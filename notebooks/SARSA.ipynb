{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addestramento Agente SARSA per Cyber Security\n",
    "\n",
    "Questo notebook implementa l'addestramento di un agente SARSA per proteggere una rete da attacchi casuali.\n",
    "\n",
    "## Come Funziona SARSA?\n",
    "1. **State (S)**: Lo stato corrente della rete\n",
    "2. **Action (A)**: L'azione di difesa scelta\n",
    "3. **Reward (R)**: La ricompensa ricevuta\n",
    "4. **State' (S')**: Il nuovo stato dopo l'azione\n",
    "5. **Action' (A')**: La prossima azione che verrà eseguita\n",
    "\n",
    "L'agente impara aggiornando i suoi valori Q basandosi sulla formula:\n",
    "Q(S,A) = Q(S,A) + α[R + γQ(S',A') - Q(S,A)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym\n",
    "pip install gym_idsgame\n",
    "pip install numpy\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie necessarie\n",
    "import gym\n",
    "import gym_idsgame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.agents.sarsa_agent import SARSAAgent\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Per i grafici più belli\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione dei parametri\n",
    "CONFIG = {\n",
    "    'num_episodes': 1000,      # Numero di episodi di training\n",
    "    'max_steps': 500,          # Passi massimi per episodio\n",
    "    'learning_rate': 0.001,    # Quanto velocemente l'agente impara\n",
    "    'gamma': 0.99,             # Importanza delle ricompense future\n",
    "    'epsilon': 1.0,            # Probabilità iniziale di esplorazione\n",
    "    'epsilon_min': 0.01,       # Probabilità minima di esplorazione\n",
    "    'epsilon_decay': 0.995     # Velocità di decadimento dell'esplorazione\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creazione dell'ambiente\n",
    "env = gym.make(\"idsgame-random-attack-v0\", num_layers=3, num_servers_per_layer=3)\n",
    "\n",
    "# Stampiamo informazioni sull'ambiente\n",
    "print(\"Informazioni sull'ambiente:\")\n",
    "print(f\"Spazio degli stati: {env.observation_space}\")\n",
    "print(f\"Spazio delle azioni: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializzazione dell'agente\n",
    "agent = SARSAAgent(\n",
    "    state_size=env.observation_space.n,\n",
    "    action_size=env.action_space.n,\n",
    "    learning_rate=CONFIG['learning_rate'],\n",
    "    gamma=CONFIG['gamma'],\n",
    "    epsilon=CONFIG['epsilon'],\n",
    "    epsilon_min=CONFIG['epsilon_min'],\n",
    "    epsilon_decay=CONFIG['epsilon_decay']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Funzione principale di training\"\"\"\n",
    "    rewards_history = []    # Lista per tracciare le ricompense\n",
    "    losses_history = []     # Lista per tracciare le losses\n",
    "    \n",
    "    # Loop principale di training\n",
    "    for episode in tqdm(range(CONFIG['num_episodes']), desc='Training'):\n",
    "        state = env.reset()  # Reset dell'ambiente\n",
    "        total_reward = 0     # Ricompensa totale per questo episodio\n",
    "        episode_losses = []  # Losses per questo episodio\n",
    "        \n",
    "        # Scegliamo la prima azione\n",
    "        action = agent.get_action(state)\n",
    "        \n",
    "        # Loop per un singolo episodio\n",
    "        for step in range(CONFIG['max_steps']):\n",
    "            # Eseguiamo l'azione nell'ambiente\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Scegliamo la prossima azione (questo è ciò che rende SARSA on-policy)\n",
    "            next_action = agent.get_action(next_state)\n",
    "            \n",
    "            # Aggiorniamo l'agente e otteniamo la loss\n",
    "            loss = agent.update(state, action, reward, next_state, next_action)\n",
    "            \n",
    "            # Aggiorniamo i valori per il prossimo step\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            total_reward += reward\n",
    "            episode_losses.append(loss)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Salviamo le statistiche dell'episodio\n",
    "        rewards_history.append(total_reward)\n",
    "        losses_history.append(np.mean(episode_losses))\n",
    "        \n",
    "        # Stampiamo le statistiche ogni 100 episodi\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"\\nEpisodio {episode + 1}\")\n",
    "            print(f\"Ricompensa media ultimi 100 episodi: {np.mean(rewards_history[-100:]):.2f}\")\n",
    "            print(f\"Epsilon corrente: {agent.epsilon:.3f}\")\n",
    "    \n",
    "    return rewards_history, losses_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avviamo il training\n",
    "print(\"Inizio training...\")\n",
    "rewards_history, losses_history = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizziamo i risultati\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot delle ricompense\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_history)\n",
    "plt.title('Ricompense per Episodio')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Ricompensa Totale')\n",
    "\n",
    "# Plot della loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(losses_history)\n",
    "plt.title('Loss Media per Episodio')\n",
    "plt.xlabel('Episodio')\n",
    "plt.ylabel('Loss Media')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salviamo il modello addestrato\n",
    "agent.save('models/sarsa_agent.npy')\n",
    "print(\"Modello salvato con successo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi dei Risultati\n",
    "\n",
    "Dopo l'addestramento, possiamo osservare:\n",
    "1. Come variano le ricompense nel tempo\n",
    "2. Come diminuisce la loss (errore di previsione)\n",
    "3. Come l'agente migliora le sue prestazioni\n",
    "\n",
    "Alcuni aspetti da considerare:\n",
    "- Un aumento delle ricompense indica che l'agente sta imparando\n",
    "- Una diminuzione della loss indica che le previsioni migliorano\n",
    "- L'epsilon che diminuisce indica meno esplorazione e più sfruttamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
